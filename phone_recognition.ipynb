{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowmason import conduct, MapReduceStep, SingletonStep, load_artifact # TODO: install this package from me: https://github.com/smfsamir/flowmason\n",
    "\n",
    "# from allosaurus.app import read_recognizer\n",
    "import panphon.distance # https://github.com/dmort27/panphon\n",
    "\n",
    "# TODO: you'll have to install these packages. Let me know if there's any trouble here; you should be able to do `pip install` for all of them\n",
    "import polars as pl\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import os\n",
    "import ipdb\n",
    "import zipfile\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import shutil\n",
    "from praatio import textgrid as tgio\n",
    "\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC \n",
    "\n",
    "\n",
    "HF_CACHE_DIR= r\"C:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\\outputs\\hf_cache_dir\"\n",
    "SCRATCH_DIR= r\"C:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\\outputs\\scratch_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABA\n",
      "[(0.03, 0.33, 'for'), (0.39, 0.62, 'the'), (0.62, 1.24, 'twentieth'), (1.52, 1.91, 'time'), (2.22, 2.65, 'that'), (2.65, 3.14, 'evening'), (3.18, 3.46, 'the'), (3.46, 3.71, 'two'), (3.71, 4.07, 'men'), (4.12, 4.48, 'shook'), (4.48, 4.87, 'hands')]\n",
      "[(0.0, 0.08, 'sil'), (0.08, 0.25, 'AO1'), (0.25, 0.41, 'TH'), (0.41, 0.59, 'ER0'), (0.59, 0.63, 'sp'), (0.63, 0.77, 'AH1'), (0.77, 0.87, 'V'), (0.87, 0.91, 'DH'), (0.91, 0.99, 'AH1'), (0.99, 1.06, 'D'), (1.06, 1.1, 'EY1'), (1.1, 1.15, 'N'), (1.15, 1.23, 'JH'), (1.23, 1.31, 'ER0'), (1.31, 1.44, 'T'), (1.44, 1.54, 'R'), (1.54, 1.63, 'EY1'), (1.63, 1.74, 'L'), (1.74, 1.84, 'sp'), (1.84, 1.94, 'F'), (1.94, 1.97, 'IH1'), (1.97, 2.0, 'L'), (2.0, 2.07, 'IH0'), (2.07, 2.13, 'P'), (2.13, 2.22, 'S'), (2.22, 2.31, 'T'), (2.31, 2.42, 'IY1'), (2.42, 2.49, 'L'), (2.49, 2.64, 'Z'), (2.64, 2.75, 'EH2'), (2.75, 2.83, 'T'), (2.83, 2.94, 'S'), (2.94, 2.97, 'EH1'), (2.97, 3.0, 'T'), (3.0, 3.03, 'ER0'), (3.03, 3.08, 'AH0'), (3.08, 3.18, 'sp')]\n",
      "[(0.0, 0.03, 'sil'), (0.03, 0.17, 'F'), (0.17, 0.25, 'AO1'), (0.25, 0.33, 'R'), (0.33, 0.39, 'sp'), (0.39, 0.57, 'DH'), (0.57, 0.62, 'AH1'), (0.62, 0.72, 'T'), (0.72, 0.84, 'W'), (0.84, 0.9, 'EH1'), (0.9, 0.97, 'N'), (0.97, 1.08, 'T'), (1.08, 1.11, 'IY0'), (1.11, 1.16, 'IH0'), (1.16, 1.24, 'TH'), (1.24, 1.52, 'sp'), (1.52, 1.61, 'T'), (1.61, 1.78, 'AY1'), (1.78, 1.91, 'M'), (1.91, 2.22, 'sp'), (2.22, 2.38, 'DH'), (2.38, 2.46, 'AE1'), (2.46, 2.65, 'T'), (2.65, 2.73, 'IY1'), (2.73, 2.83, 'V'), (2.83, 2.91, 'N'), (2.91, 2.99, 'IH0'), (2.99, 3.0911764973319493, 'NG'), (3.0911764973319493, 3.14, 'sil,K,a'), (3.14, 3.18, 'sp'), (3.18, 3.41, 'DH'), (3.41, 3.46, 'AH1'), (3.46, 3.6, 'T'), (3.6, 3.71, 'UW1'), (3.71, 3.85, 'M'), (3.85, 3.91, 'EH1'), (3.91, 4.07, 'N'), (4.07, 4.12, 'sp'), (4.12, 4.29, 'SH'), (4.29, 4.37, 'UH1'), (4.37, 4.48, 'K'), (4.48, 4.54, 'HH'), (4.54, 4.61, 'AE1'), (4.61, 4.65, 'N'), (4.65, 4.71, 'D,S,s'), (4.71, 4.87, 'Z,S,s'), (4.87, 4.9, 'sp')]\n",
      "[(array([  1,  -2,   2, ..., 310, 345, 277], dtype=int16), 'θvðdndʒtɹlflpstlztst')]\n"
     ]
    }
   ],
   "source": [
    "arpabet_to_ipa = {\n",
    "    'AA': 'ɑ', \n",
    "    'AE': 'æ',\n",
    "    'AH': 'ʌ',\n",
    "    'AO': 'ɔ',\n",
    "    'AW': 'aʊ',\n",
    "    'AY': 'aɪ',\n",
    "    'EH': 'ɛ',\n",
    "    'ER': 'ɝ',\n",
    "    'EY': 'eɪ',\n",
    "    'IH': 'ɪ',\n",
    "    'IY': 'i',\n",
    "    'OW': 'oʊ',\n",
    "    'OY': 'ɔɪ',\n",
    "    'UH': 'ʊ',\n",
    "    'UW': 'u',\n",
    "    'B': 'b',\n",
    "    'CH': 'tʃ',\n",
    "    'D': 'd',\n",
    "    'DH': 'ð',\n",
    "    'F': 'f',\n",
    "    'G': 'g',\n",
    "    'HH': 'h',\n",
    "    'JH': 'dʒ',\n",
    "    'K': 'k',\n",
    "    'L': 'l',\n",
    "    'M': 'm',\n",
    "    'N': 'n',\n",
    "    'NG': 'ŋ',\n",
    "    'P': 'p',\n",
    "    'R': 'ɹ',\n",
    "    'S': 's',\n",
    "    'SH': 'ʃ',\n",
    "    'T': 't',\n",
    "    'TH': 'θ',\n",
    "    'V': 'v',\n",
    "    'W': 'w',\n",
    "    'Y': 'j',\n",
    "    'Z': 'z',\n",
    "    'ZH': 'ʒ',\n",
    "}\n",
    "def get_data_iterator():\n",
    "    iteratable = process_zip(r'C:\\Users\\david\\OneDrive\\Desktop\\school\\URO_ARTIC_DATASET\\l2arctic_release_v5.0.zip', 1)\n",
    "    return iteratable\n",
    "\n",
    "def process_zip(zip_path, numberofspeaker = 1, numberofsampleperSpeaker = 1):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Extract all the contents into a temporary directory\n",
    "        temp_dir = 'temp_extracted'\n",
    "        zip_ref.extractall(temp_dir)\n",
    "        \n",
    "        # Getting a list of zip files (each speaker is in a separate zip file)\n",
    "        speaker_zips = [f for f in os.listdir(temp_dir) if f.endswith('.zip')]\n",
    "        \n",
    "        iteratable = []\n",
    "        countforspeaker = 0\n",
    "        \n",
    "        for speaker_zip in speaker_zips:\n",
    "            speaker_zip_path = os.path.join(temp_dir, speaker_zip)\n",
    "            speaker_temp_dir = os.path.join(temp_dir, speaker_zip.replace('.zip', ''))\n",
    "            # Extract the speaker's zip file\n",
    "            with zipfile.ZipFile(speaker_zip_path, 'r') as speaker_zip_ref:\n",
    "                speaker_zip_ref.extractall(speaker_temp_dir)\n",
    "                \n",
    "            internal_dir = os.listdir(speaker_temp_dir)[0]\n",
    "            print(internal_dir)\n",
    "            wav_dir = os.path.join(speaker_temp_dir, internal_dir, 'wav')\n",
    "            annotation_dir = os.path.join(speaker_temp_dir, internal_dir, 'annotation')\n",
    "            textgrid_dir = os.path.join(speaker_temp_dir, internal_dir, 'textgrid')\n",
    "            transcript_dir = os.path.join(speaker_temp_dir, internal_dir, 'transcript')\n",
    "            \n",
    "            if os.path.exists(wav_dir): wav_files = get_all_files(wav_dir)\n",
    "            if os.path.exists(annotation_dir): annotation_files = get_all_files(annotation_dir)\n",
    "            if os.path.exists(textgrid_dir): textgrid_files = get_all_files(textgrid_dir)\n",
    "            if os.path.exists(transcript_dir): transcript_files = get_all_files(transcript_dir)\n",
    "            \n",
    "            countforsample = 0\n",
    "            for wav_file, transcript_file, textgrid_file, annotation_file in zip(wav_files, transcript_files, textgrid_files, annotation_files):\n",
    "                words, tphones = extract_words_and_phones(textgrid_file)\n",
    "                words, aphones = extract_words_and_phones(annotation_file)\n",
    "                #list(map(lambda phone: phone[2], phones))\n",
    "                print(words)\n",
    "                print(tphones)\n",
    "                print(aphones)\n",
    "                \n",
    "                ipa_transcription = convert_arpabet_to_ipa(tphones)\n",
    "                \n",
    "                sample_rate, data = wavfile.read(wav_file)\n",
    "                with open(transcript_file, 'r') as file:\n",
    "                    iteratable.append((data, ipa_transcription))\n",
    "                    \n",
    "                countforsample+=1\n",
    "                if (countforsample == numberofsampleperSpeaker):\n",
    "                    break\n",
    "                    \n",
    "            # for wav_file in wav_files:\n",
    "            #     sample_rate, data = wavfile.read(wav_file)\n",
    "            #     #print(data)\n",
    "            #     audios.append(data)\n",
    "            \n",
    "            countforspeaker+=1\n",
    "            if (numberofspeaker == countforspeaker):\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        shutil.rmtree(temp_dir)\n",
    "        return iteratable\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "def get_all_files(directory):\n",
    "    files_and_dirs = os.listdir(directory)\n",
    "    \n",
    "    files_paths = []\n",
    "    files = [f for f in files_and_dirs if os.path.isfile(os.path.join(directory, f))]\n",
    "    \n",
    "    for file in files:\n",
    "        files_paths.append(os.path.join(directory, file))\n",
    "    \n",
    "    return files_paths\n",
    "\n",
    "\n",
    "def extract_words_and_phones(textgrid_path):\n",
    "    # Load the TextGrid file\n",
    "    tg = tgio.openTextgrid(textgrid_path, includeEmptyIntervals=False)\n",
    "\n",
    "    # Assuming the names of the tiers are 'words' and 'phones'\n",
    "    words_tier = tg.getTier('words')\n",
    "    phones_tier = tg.getTier('phones')\n",
    "\n",
    "    # Extract words and their intervals\n",
    "    words = [(interval.start, interval.end, interval.label) for interval in words_tier.entries]\n",
    "\n",
    "    # Extract phones and their intervals\n",
    "    phones = [(interval.start, interval.end, interval.label) for interval in phones_tier.entries]\n",
    "\n",
    "    return words, phones\n",
    "\n",
    "\n",
    "\n",
    "def convert_arpabet_to_ipa(arpabet_phones):\n",
    "    ipa_phones = [arpabet_to_ipa[phone[2]] for phone in arpabet_phones if phone[2] in arpabet_to_ipa]\n",
    "    return ''.join(ipa_phones)\n",
    "\n",
    "\n",
    "\n",
    "result = get_data_iterator()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "def step_generate_pfer(prediction_frame: pl.DataFrame, \n",
    "                                  **kwargs):\n",
    "    feat_edit_distance = panphon.distance.Distance().feature_edit_distance\n",
    "    # currently, the predictions have spaces between segments. Remove the spaces\n",
    "    # in the predictions column\n",
    "    prediction_frame = prediction_frame.with_columns([\n",
    "        pl.col('predictions').map_elements(lambda x: x.replace(' ', '')).alias('predictions')\n",
    "    ])\n",
    "    prediction_frame = prediction_frame.with_columns([\n",
    "        pl.struct(['predictions', 'transcripts']).map_elements(\n",
    "            lambda x: feat_edit_distance(x['predictions'], x['transcripts'])).alias('pfer')\n",
    "    ])\n",
    "    # group by the language code and compute the average pfer\n",
    "    return prediction_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_generate_predictions_notre_dame(**kwargs) -> str:\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\", cache_dir=HF_CACHE_DIR)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\", cache_dir=HF_CACHE_DIR)\n",
    "    predictions = []\n",
    "    transcripts = []\n",
    "    for datapoint in get_data_iterator(): # TODO: you have to implement this function to iterate over the arctic samples\n",
    "        audio, txt = datapoint # TODO: the iterator should returns tuples of the audio array and the transcript\n",
    "        #print(audio)\n",
    "        input_values = processor(audio, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "        # Convert input_values to double\n",
    "        input_values_double = input_values.double()\n",
    "\n",
    "        # Ensure the model is in double precision\n",
    "        model = model.double()  \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values_double).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        prediction = processor.batch_decode(predicted_ids)[0]\n",
    "        predictions.append(prediction)\n",
    "        transcripts.append(txt)\n",
    "        \n",
    "    print(pl.DataFrame({\n",
    "        \"predictions\": predictions,\n",
    "        \"transcripts\": transcripts\n",
    "    }))\n",
    "    return pl.DataFrame({\n",
    "        \"predictions\": predictions,\n",
    "        \"transcripts\": transcripts\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-09 13:12:46.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowmason.dag\u001b[0m:\u001b[36mconduct\u001b[0m:\u001b[36m314\u001b[0m - \u001b[1mStep step_generate_preds_notre_dame is cached at C:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\\outputs\\scratch_dir\\arctic_flowmason_cache\\d048277d42c6db1a98a4ff070727e347eaf6fccfc96700e07a1641e531b464b4, continuing.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[1;32mc:\\users\\david\\appdata\\local\\temp\\ipykernel_17564\\2046869975.py\u001b[0m(13)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    step_dict = OrderedDict()\n",
    "\n",
    "    step_dict['step_generate_preds_notre_dame'] = SingletonStep(step_generate_predictions_notre_dame, {\n",
    "        \"version\": \"001\"\n",
    "    })\n",
    "    \n",
    "    # step_dict['step_generate_pfer_notre_dame'] = SingletonStep(step_generate_pfer, {\n",
    "    #     'version': '001', \n",
    "    #     'prediction_frame': 'map_step_generate_notredame_preds'\n",
    "    # })\n",
    "    metadata = conduct(os.path.join(SCRATCH_DIR, \"arctic_flowmason_cache\"), step_dict, \"arctic_experiment_logs\")\n",
    "    ipdb.set_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
