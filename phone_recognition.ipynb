{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowmason import conduct, MapReduceStep, SingletonStep, load_artifact # TODO: install this package from me: https://github.com/smfsamir/flowmason\n",
    "\n",
    "# from allosaurus.app import read_recognizer\n",
    "import panphon.distance # https://github.com/dmort27/panphon\n",
    "\n",
    "# TODO: you'll have to install these packages. Let me know if there's any trouble here; you should be able to do `pip install` for all of them\n",
    "import polars as pl\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import os\n",
    "import ipdb\n",
    "import zipfile\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import shutil\n",
    "from praatio import textgrid as tgio\n",
    "import re\n",
    "import librosa\n",
    "import time\n",
    "import sounddevice as sd\n",
    "\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_root_path():\n",
    "    cwd = os.getcwd();\n",
    "    return cwd\n",
    "\n",
    "\n",
    "HF_CACHE_DIR= get_root_path() + \"\\outputs\\hf_cache_dir\"\n",
    "SCRATCH_DIR= get_root_path()+ \"\\outputs\\scratch_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arpabet_to_ipa = {\n",
    "    'AA': 'ɑ', \n",
    "    'AE': 'æ',\n",
    "    'AH': 'ʌ',\n",
    "    'AO': 'ɔ',\n",
    "    'AW': 'aʊ',\n",
    "    'AY': 'aɪ',\n",
    "    'EH': 'ɛ',\n",
    "    'ER': 'ɝ',\n",
    "    'EY': 'eɪ',\n",
    "    'IH': 'ɪ',\n",
    "    'IY': 'i',\n",
    "    'OW': 'oʊ',\n",
    "    'OY': 'ɔɪ',\n",
    "    'UH': 'ʊ',\n",
    "    'UW': 'u',\n",
    "    'B': 'b',\n",
    "    'CH': 'tʃ',\n",
    "    'D': 'd',\n",
    "    'DH': 'ð',\n",
    "    'F': 'f',\n",
    "    'G': 'g',\n",
    "    'HH': 'h',\n",
    "    'JH': 'dʒ',\n",
    "    'K': 'k',\n",
    "    'L': 'l',\n",
    "    'M': 'm',\n",
    "    'N': 'n',\n",
    "    'NG': 'ŋ',\n",
    "    'P': 'p',\n",
    "    'R': 'ɹ',\n",
    "    'S': 's',\n",
    "    'SH': 'ʃ',\n",
    "    'T': 't',\n",
    "    'TH': 'θ',\n",
    "    'V': 'v',\n",
    "    'W': 'w',\n",
    "    'Y': 'j',\n",
    "    'Z': 'z',\n",
    "    'ZH': 'ʒ',\n",
    "}\n",
    "\n",
    "\n",
    "# def get_data_iterator():\n",
    "#     cwd = os.getcwd();\n",
    "#     print(cwd)\n",
    "#     url = cwd + '\\datasets\\l2arctic_release_v5.0.zip';\n",
    "#     iteratable = process_zip(url, 1)\n",
    "#     return iteratable\n",
    "\n",
    "def process_zip(zip_path):\n",
    "    print('start processing arctic dataset:' + format(time.perf_counter()));\n",
    "    timestart = time.perf_counter();\n",
    "    difference = 0;\n",
    "    nextstart = time.perf_counter();\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Extract all the contents into a temporary directory\n",
    "        temp_dir = 'datasets'\n",
    "        zip_ref.extractall(temp_dir)\n",
    "        \n",
    "        # Getting a list of zip files (each speaker is in a separate zip file)\n",
    "        speaker_zips = [f for f in os.listdir(temp_dir) if f.endswith('.zip')]\n",
    "        \n",
    "        iteratable = []\n",
    "        countforspeaker = 0\n",
    "        \n",
    "        for speaker_zip in speaker_zips:\n",
    "            difference = time.perf_counter() - nextstart;\n",
    "            nextstart = time.perf_counter();\n",
    "            print('speaker'+ speaker_zip + ' : ' + format(difference));\n",
    "            speaker_zip_path = os.path.join(temp_dir, speaker_zip)\n",
    "            speaker_temp_dir = os.path.join(temp_dir, speaker_zip.replace('.zip', ''))\n",
    "            # Extract the speaker's zip file\n",
    "            with zipfile.ZipFile(speaker_zip_path, 'r') as speaker_zip_ref:\n",
    "                speaker_zip_ref.extractall(speaker_temp_dir)\n",
    "                \n",
    "            # internal_dir = os.listdir(speaker_temp_dir)[0]\n",
    "            # print(internal_dir)\n",
    "            # wav_dir = os.path.join(speaker_temp_dir, internal_dir, 'wav')\n",
    "            # #annotation_dir = os.path.join(speaker_temp_dir, internal_dir, 'annotation')\n",
    "            # textgrid_dir = os.path.join(speaker_temp_dir, internal_dir, 'textgrid')\n",
    "            # #transcript_dir = os.path.join(speaker_temp_dir, internal_dir, 'transcript')\n",
    "            \n",
    "            # if os.path.exists(wav_dir): wav_files = get_all_files(wav_dir)\n",
    "            # #if os.path.exists(annotation_dir): annotation_files = get_all_files(annotation_dir)\n",
    "            # if os.path.exists(textgrid_dir): textgrid_files = get_all_files(textgrid_dir)\n",
    "            # #if os.path.exists(transcript_dir): transcript_files = get_all_files(transcript_dir)\n",
    "            \n",
    "            # countforsample = 0\n",
    "            # for wav_file,  textgrid_file in zip(wav_files, textgrid_files):\n",
    "            #     words, tphones = extract_words_and_phones(textgrid_file)\n",
    "            #     # words, aphones = extract_words_and_phones(annotation_file)\n",
    "            #     #list(map(lambda phone: phone[2], phones))\n",
    "            #     print(words)\n",
    "            #     print(tphones)\n",
    "            #     #print(aphones)\n",
    "                \n",
    "            #     ipa_transcription = convert_arpabet_to_ipa(tphones)\n",
    "                \n",
    "            #     audio, sr = librosa.load(wav_file)\n",
    "            #     #with open(transcript_file, 'r') as file:\n",
    "            #     iteratable.append((audio, ipa_transcription))\n",
    "                    \n",
    "            #     countforsample+=1\n",
    "            #     if (countforsample == numberofsampleperSpeaker):\n",
    "            #         break\n",
    "                    \n",
    "            # # for wav_file in wav_files:\n",
    "            # #     sample_rate, data = wavfile.read(wav_file)\n",
    "            # #     #print(data)\n",
    "            # #     audios.append(data)\n",
    "            \n",
    "            # countforspeaker+=1\n",
    "            # if (numberofspeaker == countforspeaker):\n",
    "            #     break\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        # shutil.rmtree(temp_dir)\n",
    "        difference = time.perf_counter() - timestart;\n",
    "        \n",
    "        print('finishing processing arctic dataset: ' + format(difference));\n",
    "        \n",
    "        # return iteratable\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "def get_all_files(directory):\n",
    "    files_and_dirs = os.listdir(directory)\n",
    "    \n",
    "    files_paths = []\n",
    "    files = [f for f in files_and_dirs if os.path.isfile(os.path.join(directory, f))]\n",
    "    \n",
    "    for file in files:\n",
    "        files_paths.append(os.path.join(directory, file))\n",
    "    \n",
    "    return files_paths\n",
    "\n",
    "\n",
    "def extract_words_and_phones(textgrid_path):\n",
    "    # Load the TextGrid file\n",
    "    tg = tgio.openTextgrid(textgrid_path, includeEmptyIntervals=False)\n",
    "\n",
    "    # Assuming the names of the tiers are 'words' and 'phones'\n",
    "    words_tier = tg.getTier('words')\n",
    "    phones_tier = tg.getTier('phones')\n",
    "\n",
    "    # Extract words and their intervals\n",
    "    words = [(interval.start, interval.end, interval.label) for interval in words_tier.entries]\n",
    "\n",
    "    # Extract phones and their intervals\n",
    "    phones = [(interval.start, interval.end, re.sub(r'\\d+', '', interval.label)) for interval in phones_tier.entries]\n",
    "\n",
    "    return words, phones\n",
    "\n",
    "\n",
    "\n",
    "def convert_arpabet_to_ipa(arpabet_phones):\n",
    "    ipa_phones = [arpabet_to_ipa[phone[2]] for phone in arpabet_phones if phone[2] in arpabet_to_ipa]\n",
    "    return ''.join(ipa_phones)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\n",
      "start processing arctic dataset:51.7945574\n",
      "speakerABA.zip : 18.559541100000004\n",
      "speakerASI.zip : 5.5129412\n",
      "speakerBWC.zip : 4.606279799999996\n",
      "speakerEBVS.zip : 5.301480499999997\n",
      "speakerERMS.zip : 4.58097810000001\n",
      "speakerHJK.zip : 5.7293551000000065\n",
      "speakerHKK.zip : 4.9986715\n",
      "speakerHQTV.zip : 5.39658\n",
      "speakerl2arctic_release_v5.0.zip : 5.109118000000009\n",
      "speakerLXC.zip : 19.861714399999997\n",
      "speakerMBMPS.zip : 5.771599399999985\n",
      "speakerNCC.zip : 5.981581499999976\n",
      "speakerNJS.zip : 5.351489499999985\n",
      "speakerPNV.zip : 5.391773200000017\n",
      "speakerRRBI.zip : 5.535125899999997\n",
      "speakerSKA.zip : 5.425962900000002\n",
      "speakersuitcase_corpus.zip : 4.637012700000014\n",
      "speakerSVBI.zip : 0.9447276999999872\n",
      "speakerTHV.zip : 4.974595399999998\n",
      "speakerTLV.zip : 5.509043200000008\n",
      "speakerTNI.zip : 5.950616600000018\n",
      "speakerTXHC.zip : 5.335042499999986\n",
      "speakerYBAA.zip : 5.332261299999999\n",
      "speakerYDCK.zip : 5.499310999999977\n",
      "speakerYKWK.zip : 5.709574199999992\n",
      "speakerZHAA.zip : 5.425336500000014\n",
      "finishing processing arctic dataset: 167.6198526\n"
     ]
    }
   ],
   "source": [
    "# we only run these code when we are trying to unzip the arctic folder, \n",
    "# once the individual speaker folders are loaded in the the datasets folder we dont need to run this\n",
    "cwd = os.getcwd();\n",
    "print(cwd)\n",
    "url = cwd + '\\datasets\\l2arctic_release_v5.0.zip';\n",
    "iteratable = process_zip(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing arctic dataset:219.4367074\n",
      "speakerc:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\\datasets\\ABA : 0.0067677000000117005\n",
      "ABA\n",
      "[(0.08, 0.59, 'author'), (0.63, 0.87, 'of'), (0.87, 0.99, 'the'), (0.99, 1.31, 'danger'), (1.31, 1.74, 'trail'), (1.84, 2.13, 'philip'), (2.13, 2.64, 'steels'), (2.64, 3.08, 'etc')]\n",
      "finishing processing arctic dataset: 3.7056856000000096\n",
      "[(array([-3.5088185e-06,  5.3007375e-06, -6.7105429e-07, ...,\n",
      "        1.0927755e-02,  7.3688738e-03,  9.6428636e-03], dtype=float32), 'ɔθɝʌvðʌdeɪndʒɝtɹeɪlfɪlɪpstilzɛtsɛtɝʌ')]\n"
     ]
    }
   ],
   "source": [
    "def get_data_iterator():\n",
    "    cwd = os.getcwd();\n",
    "    url = cwd + '\\datasets';\n",
    "    iteratable = process_datasets(url)\n",
    "    return iteratable\n",
    "\n",
    "def process_datasets(data_path, numberofspeaker = 1, numberofsampleperSpeaker = 1):\n",
    "    print('start processing arctic dataset:' + format(time.perf_counter()));\n",
    "    timestart = time.perf_counter();\n",
    "    difference = 0;\n",
    "    nextstart = time.perf_counter();\n",
    "    \n",
    "    # Get a list of all items in 'data_path'\n",
    "    all_items = os.listdir(data_path)\n",
    "\n",
    "    # Filter for directories only, excluding any zip files or other file types\n",
    "    speaker_folders = [os.path.join(data_path, f) for f in all_items if os.path.isdir(os.path.join(data_path, f)) and not f.endswith('.zip')]\n",
    "        \n",
    "    iteratable = []\n",
    "    countforspeaker = 0\n",
    "   \n",
    "        \n",
    "    for speaker_folder in speaker_folders:\n",
    "        difference = time.perf_counter() - nextstart;\n",
    "        nextstart = time.perf_counter();\n",
    "        print('speaker'+ speaker_folder + ' : ' + format(difference));\n",
    "        internal_dir = os.listdir(speaker_folder)[0]\n",
    "        print(internal_dir)\n",
    "        wav_dir = os.path.join(speaker_folder, internal_dir, 'wav')\n",
    "            \n",
    "        textgrid_dir = os.path.join(speaker_folder, internal_dir, 'textgrid')\n",
    "       \n",
    "        if os.path.exists(wav_dir): wav_files = get_all_files(wav_dir)\n",
    "            \n",
    "        if os.path.exists(textgrid_dir): textgrid_files = get_all_files(textgrid_dir)\n",
    "            \n",
    "            \n",
    "        \n",
    "        countforsample = 0\n",
    "        for wav_file,  textgrid_file in zip(wav_files, textgrid_files):\n",
    "            words, tphones = extract_words_and_phones(textgrid_file)\n",
    "            ipa_transcription = convert_arpabet_to_ipa(tphones)\n",
    "                \n",
    "            print(words)\n",
    "            audio, sr = librosa.load(wav_file, sr=16000) # because the model was trained using a sampling rate of 16000. Please make sure that the provided `raw_speech` input was sampled with 16000\n",
    "            \n",
    "            # sd.play(audio, sr)\n",
    "            # print(sr)\n",
    "            \n",
    "            # sd.wait\n",
    "                \n",
    "            iteratable.append((audio, ipa_transcription))\n",
    "                    \n",
    "            countforsample+=1\n",
    "            if (countforsample == numberofsampleperSpeaker):\n",
    "                break\n",
    "        \n",
    "            \n",
    "        countforspeaker+=1\n",
    "        if (numberofspeaker == countforspeaker):\n",
    "            break\n",
    "                \n",
    "                \n",
    "    difference = time.perf_counter() - timestart;\n",
    "        \n",
    "    print('finishing processing arctic dataset: ' + format(difference));\n",
    "        \n",
    "    return iteratable\n",
    "\n",
    "\n",
    "result = get_data_iterator();\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "def step_generate_pfer(prediction_frame: pl.DataFrame, \n",
    "                                  **kwargs):\n",
    "    feat_edit_distance = panphon.distance.Distance().feature_edit_distance\n",
    "    # currently, the predictions have spaces between segments. Remove the spaces\n",
    "    # in the predictions column\n",
    "    prediction_frame = prediction_frame.with_columns([\n",
    "        pl.col('predictions').map_elements(lambda x: x.replace(' ', '')).alias('predictions')\n",
    "    ])\n",
    "    prediction_frame = prediction_frame.with_columns([\n",
    "        pl.struct(['predictions', 'transcripts']).map_elements(\n",
    "            lambda x: feat_edit_distance(x['predictions'], x['transcripts'])).alias('pfer')\n",
    "    ])\n",
    "    # group by the language code and compute the average pfer\n",
    "    return prediction_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing arctic dataset:226.0525429\n",
      "speakerc:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\\datasets\\ABA : 0.015240500000004431\n",
      "ABA\n",
      "[(0.08, 0.59, 'author'), (0.63, 0.87, 'of'), (0.87, 0.99, 'the'), (0.99, 1.31, 'danger'), (1.31, 1.74, 'trail'), (1.84, 2.13, 'philip'), (2.13, 2.64, 'steels'), (2.64, 3.08, 'etc')]\n",
      "finishing processing arctic dataset: 0.26756829999999354\n",
      "['usɔɾɔvdadɨnd͡ʑɛɾtɾajalflibstilzɛkstɾa']\n",
      "['ɔθɝʌvðʌdeɪndʒɝtɹeɪlfɪlɪpstilzɛtsɛtɝʌ']\n",
      "shape: (1, 2)\n",
      "┌──────────────────────────────────┬───────────────────────────────────┐\n",
      "│ predictions                      ┆ transcripts                       │\n",
      "│ ---                              ┆ ---                               │\n",
      "│ str                              ┆ str                               │\n",
      "╞══════════════════════════════════╪═══════════════════════════════════╡\n",
      "│ usɔɾɔvdadɨnd͡ʑɛɾtɾajalflibstilzɛ… ┆ ɔθɝʌvðʌdeɪndʒɝtɹeɪlfɪlɪpstilzɛts… │\n",
      "└──────────────────────────────────┴───────────────────────────────────┘\n",
      "shape: (1, 3)\n",
      "┌──────────────────────────────────┬───────────────────────────────────┬──────────┐\n",
      "│ predictions                      ┆ transcripts                       ┆ pfer     │\n",
      "│ ---                              ┆ ---                               ┆ ---      │\n",
      "│ str                              ┆ str                               ┆ f64      │\n",
      "╞══════════════════════════════════╪═══════════════════════════════════╪══════════╡\n",
      "│ usɔɾɔvdadɨnd͡ʑɛɾtɾajalflibstilzɛ… ┆ ɔθɝʌvðʌdeɪndʒɝtɹeɪlfɪlɪpstilzɛts… ┆ 5.270833 │\n",
      "└──────────────────────────────────┴───────────────────────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "def step_generate_predictions_notre_dame(**kwargs) -> str:\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\", cache_dir=HF_CACHE_DIR)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\", cache_dir=HF_CACHE_DIR)\n",
    "    predictions = []\n",
    "    transcripts = []\n",
    "    for datapoint in get_data_iterator(): # TODO: you have to implement this function to iterate over the arctic samples\n",
    "        audio, txt = datapoint # TODO: the iterator should returns tuples of the audio array and the transcript\n",
    "        # print(audio)\n",
    "        # sd.play(audio, sr)\n",
    "        # sd.wait()\n",
    "        # print(txt)\n",
    "        input_values = processor(audio, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "        # Convert input_values to double\n",
    "        #input_values_double = input_values.double()\n",
    "\n",
    "        # Ensure the model is in double precision\n",
    "        #model = model.double()  \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        prediction = processor.batch_decode(predicted_ids)[0]\n",
    "        predictions.append(prediction)\n",
    "        transcripts.append(txt)\n",
    "\n",
    "    \n",
    "    print(predictions);\n",
    "    print(transcripts);\n",
    "    return pl.DataFrame({\n",
    "        \"predictions\": predictions,\n",
    "        \"transcripts\": transcripts\n",
    "    })\n",
    "\n",
    "\n",
    "result = step_generate_predictions_notre_dame()\n",
    "result2 = step_generate_pfer(result)\n",
    "print(result)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-10 20:31:41.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowmason.dag\u001b[0m:\u001b[36mconduct\u001b[0m:\u001b[36m314\u001b[0m - \u001b[1mStep step_generate_preds_notre_dame is cached at c:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\\outputs\\scratch_dir\\arctic_flowmason_cache\\d048277d42c6db1a98a4ff070727e347eaf6fccfc96700e07a1641e531b464b4, continuing.\u001b[0m\n",
      "\u001b[32m2024-03-10 20:31:41.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowmason.dag\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m168\u001b[0m - \u001b[1mRunning step step_generate_pfer_notre_dame\u001b[0m\n",
      "\u001b[32m2024-03-10 20:31:41.757\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mflowmason.dag\u001b[0m:\u001b[36mconduct\u001b[0m:\u001b[36m346\u001b[0m - \u001b[31m\u001b[1mError occurred while running step step_generate_pfer_notre_dame: 'str' object has no attribute 'with_columns'\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'with_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m step_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_generate_preds_notre_dame\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m SingletonStep(step_generate_predictions_notre_dame, {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m001\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m })\n\u001b[0;32m      8\u001b[0m step_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_generate_pfer_notre_dame\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m SingletonStep(step_generate_pfer, {\n\u001b[0;32m      9\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m001\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     10\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_frame\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap_step_generate_notredame_preds\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m })\n\u001b[1;32m---> 12\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43mconduct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSCRATCH_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marctic_flowmason_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marctic_experiment_logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m ipdb\u001b[38;5;241m.\u001b[39mset_trace()\n",
      "File \u001b[1;32mc:\\users\\david\\onedrive\\desktop\\school\\flowmason\\flowmason\\flowmason\\dag.py:352\u001b[0m, in \u001b[0;36mconduct\u001b[1;34m(cache_dir, experiment_steps, experiment_name)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(run_fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    351\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(steps_metadata, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# write the metadata to a json file.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(run_fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\users\\david\\onedrive\\desktop\\school\\flowmason\\flowmason\\flowmason\\dag.py:328\u001b[0m, in \u001b[0;36mconduct\u001b[1;34m(cache_dir, experiment_steps, experiment_name)\u001b[0m\n\u001b[0;32m    326\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    327\u001b[0m step_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m exp_step_name\n\u001b[1;32m--> 328\u001b[0m result_cache_path, execution_status \u001b[38;5;241m=\u001b[39m \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    330\u001b[0m metadata \u001b[38;5;241m=\u001b[39m create_metadata(step_version, step_kwargs, start_time, end_time,\n\u001b[0;32m    331\u001b[0m                         cache_dir, execution_status)\n",
      "File \u001b[1;32mc:\\users\\david\\onedrive\\desktop\\school\\flowmason\\flowmason\\flowmason\\dag.py:177\u001b[0m, in \u001b[0;36mstep_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m cache_map: \u001b[38;5;66;03m# substitute the value with the result of the step.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m         kwargs[key] \u001b[38;5;241m=\u001b[39m dill\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(cache_map[value], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m--> 177\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mstep_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     cache_path \u001b[38;5;241m=\u001b[39m cache_result(cache_dir, step_name, step_version, original_kwargs, result)\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mstep_generate_pfer\u001b[1;34m(prediction_frame, **kwargs)\u001b[0m\n\u001b[0;32m      4\u001b[0m feat_edit_distance \u001b[38;5;241m=\u001b[39m panphon\u001b[38;5;241m.\u001b[39mdistance\u001b[38;5;241m.\u001b[39mDistance()\u001b[38;5;241m.\u001b[39mfeature_edit_distance\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# currently, the predictions have spaces between segments. Remove the spaces\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# in the predictions column\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m prediction_frame \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m([\n\u001b[0;32m      8\u001b[0m     pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmap_elements(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m prediction_frame \u001b[38;5;241m=\u001b[39m prediction_frame\u001b[38;5;241m.\u001b[39mwith_columns([\n\u001b[0;32m     11\u001b[0m     pl\u001b[38;5;241m.\u001b[39mstruct([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscripts\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmap_elements(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: feat_edit_distance(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscripts\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpfer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m ])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# group by the language code and compute the average pfer\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'with_columns'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    step_dict = OrderedDict()\n",
    "\n",
    "    step_dict['step_generate_preds_notre_dame'] = SingletonStep(step_generate_predictions_notre_dame, {\n",
    "        \"version\": \"001\"\n",
    "    })\n",
    "    \n",
    "    step_dict['step_generate_pfer_notre_dame'] = SingletonStep(step_generate_pfer, {\n",
    "         'version': '001', \n",
    "         'prediction_frame': 'map_step_generate_notredame_preds'\n",
    "    })\n",
    "    metadata = conduct(os.path.join(SCRATCH_DIR, \"arctic_flowmason_cache\"), step_dict, \"arctic_experiment_logs\")\n",
    "    ipdb.set_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
