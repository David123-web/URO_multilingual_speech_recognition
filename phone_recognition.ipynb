{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowmason import conduct, MapReduceStep, SingletonStep, load_artifact # TODO: install this package from me: https://github.com/smfsamir/flowmason\n",
    "\n",
    "# from allosaurus.app import read_recognizer\n",
    "import panphon.distance # https://github.com/dmort27/panphon\n",
    "\n",
    "# TODO: you'll have to install these packages. Let me know if there's any trouble here; you should be able to do `pip install` for all of them\n",
    "import polars as pl\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import os\n",
    "import ipdb\n",
    "import zipfile\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import shutil\n",
    "from praatio import textgrid as tgio\n",
    "import re\n",
    "import librosa\n",
    "import time\n",
    "\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_root_path():\n",
    "    cwd = os.getcwd();\n",
    "    return cwd\n",
    "\n",
    "\n",
    "HF_CACHE_DIR= get_root_path() + \"\\outputs\\hf_cache_dir\"\n",
    "SCRATCH_DIR= get_root_path()+ \"\\outputs\\scratch_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\n",
      "start processing arctic dataset:13685.7532307\n",
      "speakerABA.zip : 20.947451800000636\n",
      "speakerASI.zip : 7.639224499998818\n",
      "speakerBWC.zip : 5.742717700000867\n",
      "speakerEBVS.zip : 6.897728500000085\n",
      "speakerERMS.zip : 5.812350300000617\n",
      "speakerHJK.zip : 6.4035198999990826\n",
      "speakerHKK.zip : 5.908732500000042\n",
      "speakerHQTV.zip : 6.157685899999706\n",
      "speakerl2arctic_release_v5.0.zip : 5.987224500000593\n",
      "speakerLXC.zip : 20.447983600000953\n",
      "speakerMBMPS.zip : 5.686578299999383\n",
      "speakerNCC.zip : 5.5227562000000034\n",
      "speakerNJS.zip : 5.228591499999311\n",
      "speakerPNV.zip : 4.838991399999941\n",
      "speakerRRBI.zip : 5.190570900000239\n",
      "speakerSKA.zip : 5.079096100000243\n",
      "speakersuitcase_corpus.zip : 4.318308500000057\n",
      "speakerSVBI.zip : 0.974261999999726\n",
      "speakerTHV.zip : 5.060335399999531\n",
      "speakerTLV.zip : 5.177257199999076\n",
      "speakerTNI.zip : 5.683974000001399\n",
      "speakerTXHC.zip : 5.913738400000511\n",
      "speakerYBAA.zip : 6.67678479999995\n",
      "speakerYDCK.zip : 6.425396299999193\n",
      "speakerYKWK.zip : 5.769626600000265\n",
      "speakerZHAA.zip : 5.783208299999387\n",
      "finishing processing arctic dataset: 181.32537260000026\n"
     ]
    }
   ],
   "source": [
    "arpabet_to_ipa = {\n",
    "    'AA': 'ɑ', \n",
    "    'AE': 'æ',\n",
    "    'AH': 'ʌ',\n",
    "    'AO': 'ɔ',\n",
    "    'AW': 'aʊ',\n",
    "    'AY': 'aɪ',\n",
    "    'EH': 'ɛ',\n",
    "    'ER': 'ɝ',\n",
    "    'EY': 'eɪ',\n",
    "    'IH': 'ɪ',\n",
    "    'IY': 'i',\n",
    "    'OW': 'oʊ',\n",
    "    'OY': 'ɔɪ',\n",
    "    'UH': 'ʊ',\n",
    "    'UW': 'u',\n",
    "    'B': 'b',\n",
    "    'CH': 'tʃ',\n",
    "    'D': 'd',\n",
    "    'DH': 'ð',\n",
    "    'F': 'f',\n",
    "    'G': 'g',\n",
    "    'HH': 'h',\n",
    "    'JH': 'dʒ',\n",
    "    'K': 'k',\n",
    "    'L': 'l',\n",
    "    'M': 'm',\n",
    "    'N': 'n',\n",
    "    'NG': 'ŋ',\n",
    "    'P': 'p',\n",
    "    'R': 'ɹ',\n",
    "    'S': 's',\n",
    "    'SH': 'ʃ',\n",
    "    'T': 't',\n",
    "    'TH': 'θ',\n",
    "    'V': 'v',\n",
    "    'W': 'w',\n",
    "    'Y': 'j',\n",
    "    'Z': 'z',\n",
    "    'ZH': 'ʒ',\n",
    "}\n",
    "\n",
    "\n",
    "# def get_data_iterator():\n",
    "#     cwd = os.getcwd();\n",
    "#     print(cwd)\n",
    "#     url = cwd + '\\datasets\\l2arctic_release_v5.0.zip';\n",
    "#     iteratable = process_zip(url, 1)\n",
    "#     return iteratable\n",
    "\n",
    "def process_zip(zip_path):\n",
    "    print('start processing arctic dataset:' + format(time.perf_counter()));\n",
    "    timestart = time.perf_counter();\n",
    "    difference = 0;\n",
    "    nextstart = time.perf_counter();\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Extract all the contents into a temporary directory\n",
    "        temp_dir = 'datasets'\n",
    "        zip_ref.extractall(temp_dir)\n",
    "        \n",
    "        # Getting a list of zip files (each speaker is in a separate zip file)\n",
    "        speaker_zips = [f for f in os.listdir(temp_dir) if f.endswith('.zip')]\n",
    "        \n",
    "        iteratable = []\n",
    "        countforspeaker = 0\n",
    "        \n",
    "        for speaker_zip in speaker_zips:\n",
    "            difference = time.perf_counter() - nextstart;\n",
    "            nextstart = time.perf_counter();\n",
    "            print('speaker'+ speaker_zip + ' : ' + format(difference));\n",
    "            speaker_zip_path = os.path.join(temp_dir, speaker_zip)\n",
    "            speaker_temp_dir = os.path.join(temp_dir, speaker_zip.replace('.zip', ''))\n",
    "            # Extract the speaker's zip file\n",
    "            with zipfile.ZipFile(speaker_zip_path, 'r') as speaker_zip_ref:\n",
    "                speaker_zip_ref.extractall(speaker_temp_dir)\n",
    "                \n",
    "            # internal_dir = os.listdir(speaker_temp_dir)[0]\n",
    "            # print(internal_dir)\n",
    "            # wav_dir = os.path.join(speaker_temp_dir, internal_dir, 'wav')\n",
    "            # #annotation_dir = os.path.join(speaker_temp_dir, internal_dir, 'annotation')\n",
    "            # textgrid_dir = os.path.join(speaker_temp_dir, internal_dir, 'textgrid')\n",
    "            # #transcript_dir = os.path.join(speaker_temp_dir, internal_dir, 'transcript')\n",
    "            \n",
    "            # if os.path.exists(wav_dir): wav_files = get_all_files(wav_dir)\n",
    "            # #if os.path.exists(annotation_dir): annotation_files = get_all_files(annotation_dir)\n",
    "            # if os.path.exists(textgrid_dir): textgrid_files = get_all_files(textgrid_dir)\n",
    "            # #if os.path.exists(transcript_dir): transcript_files = get_all_files(transcript_dir)\n",
    "            \n",
    "            # countforsample = 0\n",
    "            # for wav_file,  textgrid_file in zip(wav_files, textgrid_files):\n",
    "            #     words, tphones = extract_words_and_phones(textgrid_file)\n",
    "            #     # words, aphones = extract_words_and_phones(annotation_file)\n",
    "            #     #list(map(lambda phone: phone[2], phones))\n",
    "            #     print(words)\n",
    "            #     print(tphones)\n",
    "            #     #print(aphones)\n",
    "                \n",
    "            #     ipa_transcription = convert_arpabet_to_ipa(tphones)\n",
    "                \n",
    "            #     audio, sr = librosa.load(wav_file)\n",
    "            #     #with open(transcript_file, 'r') as file:\n",
    "            #     iteratable.append((audio, ipa_transcription))\n",
    "                    \n",
    "            #     countforsample+=1\n",
    "            #     if (countforsample == numberofsampleperSpeaker):\n",
    "            #         break\n",
    "                    \n",
    "            # # for wav_file in wav_files:\n",
    "            # #     sample_rate, data = wavfile.read(wav_file)\n",
    "            # #     #print(data)\n",
    "            # #     audios.append(data)\n",
    "            \n",
    "            # countforspeaker+=1\n",
    "            # if (numberofspeaker == countforspeaker):\n",
    "            #     break\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        # shutil.rmtree(temp_dir)\n",
    "        difference = time.perf_counter() - timestart;\n",
    "        \n",
    "        print('finishing processing arctic dataset: ' + format(difference));\n",
    "        \n",
    "        # return iteratable\n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "def get_all_files(directory):\n",
    "    files_and_dirs = os.listdir(directory)\n",
    "    \n",
    "    files_paths = []\n",
    "    files = [f for f in files_and_dirs if os.path.isfile(os.path.join(directory, f))]\n",
    "    \n",
    "    for file in files:\n",
    "        files_paths.append(os.path.join(directory, file))\n",
    "    \n",
    "    return files_paths\n",
    "\n",
    "\n",
    "def extract_words_and_phones(textgrid_path):\n",
    "    # Load the TextGrid file\n",
    "    tg = tgio.openTextgrid(textgrid_path, includeEmptyIntervals=False)\n",
    "\n",
    "    # Assuming the names of the tiers are 'words' and 'phones'\n",
    "    words_tier = tg.getTier('words')\n",
    "    phones_tier = tg.getTier('phones')\n",
    "\n",
    "    # Extract words and their intervals\n",
    "    words = [(interval.start, interval.end, interval.label) for interval in words_tier.entries]\n",
    "\n",
    "    # Extract phones and their intervals\n",
    "    phones = [(interval.start, interval.end, re.sub(r'\\d+', '', interval.label)) for interval in phones_tier.entries]\n",
    "\n",
    "    return words, phones\n",
    "\n",
    "\n",
    "\n",
    "def convert_arpabet_to_ipa(arpabet_phones):\n",
    "    ipa_phones = [arpabet_to_ipa[phone[2]] for phone in arpabet_phones if phone[2] in arpabet_to_ipa]\n",
    "    return ''.join(ipa_phones)\n",
    "\n",
    "\n",
    "cwd = os.getcwd();\n",
    "print(cwd)\n",
    "url = cwd + '\\datasets\\l2arctic_release_v5.0.zip';\n",
    "iteratable = process_zip(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing arctic dataset:13883.4263684\n",
      "speakerc:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\\datasets\\ABA : 0.0051937999996880535\n",
      "ABA\n",
      "finishing processing arctic dataset: 0.34489250000115135\n",
      "[(array([ 1.1901691e-06, -6.8628915e-06,  1.5418029e-05, ...,\n",
      "        7.9195611e-03,  7.6785325e-03,  1.0348516e-02], dtype=float32), 'ɔθɝʌvðʌdeɪndʒɝtɹeɪlfɪlɪpstilzɛtsɛtɝʌ')]\n"
     ]
    }
   ],
   "source": [
    "def get_data_iterator():\n",
    "    cwd = os.getcwd();\n",
    "    url = cwd + '\\datasets';\n",
    "    iteratable = process_datasets(url)\n",
    "    return iteratable\n",
    "\n",
    "def process_datasets(data_path, numberofspeaker = 1, numberofsampleperSpeaker = 1):\n",
    "    print('start processing arctic dataset:' + format(time.perf_counter()));\n",
    "    timestart = time.perf_counter();\n",
    "    difference = 0;\n",
    "    nextstart = time.perf_counter();\n",
    "    \n",
    "    # Get a list of all items in 'data_path'\n",
    "    all_items = os.listdir(data_path)\n",
    "\n",
    "    # Filter for directories only, excluding any zip files or other file types\n",
    "    speaker_folders = [os.path.join(data_path, f) for f in all_items if os.path.isdir(os.path.join(data_path, f)) and not f.endswith('.zip')]\n",
    "        \n",
    "    iteratable = []\n",
    "    countforspeaker = 0\n",
    "   \n",
    "        \n",
    "    for speaker_folder in speaker_folders:\n",
    "        difference = time.perf_counter() - nextstart;\n",
    "        nextstart = time.perf_counter();\n",
    "        print('speaker'+ speaker_folder + ' : ' + format(difference));\n",
    "        internal_dir = os.listdir(speaker_folder)[0]\n",
    "        print(internal_dir)\n",
    "        wav_dir = os.path.join(speaker_folder, internal_dir, 'wav')\n",
    "            \n",
    "        textgrid_dir = os.path.join(speaker_folder, internal_dir, 'textgrid')\n",
    "       \n",
    "        if os.path.exists(wav_dir): wav_files = get_all_files(wav_dir)\n",
    "            \n",
    "        if os.path.exists(textgrid_dir): textgrid_files = get_all_files(textgrid_dir)\n",
    "            \n",
    "            \n",
    "        \n",
    "        countforsample = 0\n",
    "        for wav_file,  textgrid_file in zip(wav_files, textgrid_files):\n",
    "            words, tphones = extract_words_and_phones(textgrid_file)\n",
    "            ipa_transcription = convert_arpabet_to_ipa(tphones)\n",
    "                \n",
    "            audio, sr = librosa.load(wav_file)\n",
    "                \n",
    "            iteratable.append((audio, ipa_transcription))\n",
    "                    \n",
    "            countforsample+=1\n",
    "            if (countforsample == numberofsampleperSpeaker):\n",
    "                break\n",
    "        \n",
    "            \n",
    "        countforspeaker+=1\n",
    "        if (numberofspeaker == countforspeaker):\n",
    "            break\n",
    "                \n",
    "                \n",
    "    difference = time.perf_counter() - timestart;\n",
    "        \n",
    "    print('finishing processing arctic dataset: ' + format(difference));\n",
    "        \n",
    "    return iteratable\n",
    "\n",
    "\n",
    "result = get_data_iterator();\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "def step_generate_pfer(prediction_frame: pl.DataFrame, \n",
    "                                  **kwargs):\n",
    "    feat_edit_distance = panphon.distance.Distance().feature_edit_distance\n",
    "    # currently, the predictions have spaces between segments. Remove the spaces\n",
    "    # in the predictions column\n",
    "    prediction_frame = prediction_frame.with_columns([\n",
    "        pl.col('predictions').map_elements(lambda x: x.replace(' ', '')).alias('predictions')\n",
    "    ])\n",
    "    prediction_frame = prediction_frame.with_columns([\n",
    "        pl.struct(['predictions', 'transcripts']).map_elements(\n",
    "            lambda x: feat_edit_distance(x['predictions'], x['transcripts'])).alias('pfer')\n",
    "    ])\n",
    "    # group by the language code and compute the average pfer\n",
    "    return prediction_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing arctic dataset:13899.929499\n",
      "speakerc:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\\datasets\\ABA : 0.004579200000080164\n",
      "ABA\n",
      "finishing processing arctic dataset: 0.13926989999890793\n",
      "shape: (1, 2)\n",
      "┌──────────────────────────────────┬───────────────────────────────────┐\n",
      "│ predictions                      ┆ transcripts                       │\n",
      "│ ---                              ┆ ---                               │\n",
      "│ str                              ┆ str                               │\n",
      "╞══════════════════════════════════╪═══════════════════════════════════╡\n",
      "│ jisuɾuvgudynd͡ʒuɾtɾojomfulibisti… ┆ ɔθɝʌvðʌdeɪndʒɝtɹeɪlfɪlɪpstilzɛts… │\n",
      "└──────────────────────────────────┴───────────────────────────────────┘\n",
      "['jisuɾuvgudynd͡ʒuɾtɾojomfulibistilinztstɾu']\n",
      "['ɔθɝʌvðʌdeɪndʒɝtɹeɪlfɪlɪpstilzɛtsɛtɝʌ']\n",
      "shape: (1, 2)\n",
      "┌──────────────────────────────────┬───────────────────────────────────┐\n",
      "│ predictions                      ┆ transcripts                       │\n",
      "│ ---                              ┆ ---                               │\n",
      "│ str                              ┆ str                               │\n",
      "╞══════════════════════════════════╪═══════════════════════════════════╡\n",
      "│ jisuɾuvgudynd͡ʒuɾtɾojomfulibisti… ┆ ɔθɝʌvðʌdeɪndʒɝtɹeɪlfɪlɪpstilzɛts… │\n",
      "└──────────────────────────────────┴───────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "def step_generate_predictions_notre_dame(**kwargs) -> str:\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\", cache_dir=HF_CACHE_DIR)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\", cache_dir=HF_CACHE_DIR)\n",
    "    predictions = []\n",
    "    transcripts = []\n",
    "    for datapoint in get_data_iterator(): # TODO: you have to implement this function to iterate over the arctic samples\n",
    "        audio, txt = datapoint # TODO: the iterator should returns tuples of the audio array and the transcript\n",
    "        #print(audio)\n",
    "        input_values = processor(audio, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "        # Convert input_values to double\n",
    "        #input_values_double = input_values.double()\n",
    "\n",
    "        # Ensure the model is in double precision\n",
    "        #model = model.double()  \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        prediction = processor.batch_decode(predicted_ids)[0]\n",
    "        predictions.append(prediction)\n",
    "        transcripts.append(txt)\n",
    "        \n",
    "    print(pl.DataFrame({\n",
    "        \"predictions\": predictions,\n",
    "        \"transcripts\": transcripts\n",
    "    }))\n",
    "    \n",
    "    print(predictions);\n",
    "    print(transcripts);\n",
    "    return pl.DataFrame({\n",
    "        \"predictions\": predictions,\n",
    "        \"transcripts\": transcripts\n",
    "    })\n",
    "\n",
    "\n",
    "result = step_generate_predictions_notre_dame()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-23 14:00:31.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mflowmason.dag\u001b[0m:\u001b[36mconduct\u001b[0m:\u001b[36m314\u001b[0m - \u001b[1mStep step_generate_preds_notre_dame is cached at C:\\Users\\david\\OneDrive\\Desktop\\school\\URO\\URO_multilingual_speech_recognition\\outputs\\scratch_dir\\arctic_flowmason_cache\\d048277d42c6db1a98a4ff070727e347eaf6fccfc96700e07a1641e531b464b4, continuing.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[1;32mc:\\users\\david\\appdata\\local\\temp\\ipykernel_12192\\2046869975.py\u001b[0m(13)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    step_dict = OrderedDict()\n",
    "\n",
    "    step_dict['step_generate_preds_notre_dame'] = SingletonStep(step_generate_predictions_notre_dame, {\n",
    "        \"version\": \"001\"\n",
    "    })\n",
    "    \n",
    "    # step_dict['step_generate_pfer_notre_dame'] = SingletonStep(step_generate_pfer, {\n",
    "    #     'version': '001', \n",
    "    #     'prediction_frame': 'map_step_generate_notredame_preds'\n",
    "    # })\n",
    "    metadata = conduct(os.path.join(SCRATCH_DIR, \"arctic_flowmason_cache\"), step_dict, \"arctic_experiment_logs\")\n",
    "    ipdb.set_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
